---
title: "Mobile + Autonomous Discovery Implementation"
description: "Detailed implementation plan for mobile pentesting and hypothesis-driven vulnerability discovery, grounded in Esprit's current web pipeline"
---

## Purpose

This document defines two implementation tracks:

1. Mobile app pentest support (Android/iOS focused).
2. Autonomous vulnerability discovery (hypothesis-driven, not only instruction-following).

It starts with a detailed as-is walkthrough of the current web pentest pipeline so design changes stay compatible with existing behavior.

## Current Web Pentest Pipeline (As-Is)

### 1) CLI and Target Intake

Current flow starts in `esprit/interface/main.py`.

- CLI target parsing supports only:
  - `web_application`
  - `repository`
  - `local_code`
  - `ip_address`
- Classification logic is in `esprit/interface/utils.py` (`infer_target_type`).
- Target metadata is normalized into `targets_info`, then workspace subdirs are assigned for repositories/local source.
- Repository targets are cloned pre-scan.

Result: the scan context is built around web/API and source-repo assets. There is no first-class `mobile_app` target type.

### 2) Runtime Selection and Sandbox Boot

Runtime selection happens before agent execution.

- `cloud` runtime for paid Esprit subscriptions using Esprit models.
- `docker` runtime otherwise.

Root agent sandbox init happens in `BaseAgent._initialize_sandbox_and_state()`.

For Docker runtime (`esprit/runtime/docker_runtime.py`):

- Reuses one scan container (`esprit-scan-<scan_id>`) across all agents.
- Starts a tool server in-container.
- Registers each agent with the tool server.
- Copies local source directories into `/workspace` once per scan.

### 3) Container Startup and Traffic Interception

`containers/docker-entrypoint.sh` sets up core web testing infrastructure.

- Starts Caido CLI on `127.0.0.1:48080`.
- Creates and selects a temporary Caido project.
- Sets system-wide proxy env vars (`http_proxy`, `https_proxy`, etc.) to Caido.
- Injects trusted CA cert into system/browser trust stores.
- Starts FastAPI tool server (`esprit.runtime.tool_server`) with auth token.

This is why browser traffic and many command-line HTTP tools are visible through proxy tooling.

### 4) Agent Loop and LLM Tool Calling

`BaseAgent.agent_loop()` is the execution kernel.

Per iteration:

1. Build messages via `LLM._prepare_messages()`:
   - System prompt (`esprit/agents/EspritAgent/system_prompt.jinja`)
   - Agent identity block
   - Compressed conversation history (`MemoryCompressor`)
2. Generate model output (`LLM.generate()`), using:
   - Native tool calling when supported, or
   - XML `<function=...>` fallback parsing.
3. Append assistant response to state.
4. Execute tool invocations via `process_tool_invocations()`.

Tool execution routing (`esprit/tools/executor.py`):

- Sandbox tools: proxied to in-container `/execute` endpoint.
- Non-sandbox tools: execute on host process (for orchestration/reporting).

### 5) Current Web Toolchain

Core tools and behavior:

- `browser_action`:
  - Playwright Chromium automation.
  - Per-agent browser isolation keyed by agent id.
  - Screenshots returned with each state update.
- Proxy tools (`list_requests`, `view_request`, `repeat_request`, `send_request`, sitemap/scope):
  - Caido GraphQL queries for captured traffic and sitemap.
  - Replay and mutate requests through proxy.
- `terminal_execute`:
  - Per-agent persistent tmux shell in `/workspace`.
- `python_action`:
  - Per-agent persistent IPython session.
  - Proxy functions preloaded for analysis and replay workflows.

This combination is already a strong web/API pentest substrate.

### 6) Multi-Agent Orchestration

Agent orchestration is local-side (`esprit/tools/agents_graph/agents_graph_actions.py`).

- Root and subagents are represented as graph nodes.
- `create_agent` runs subagents in threads, with inherited context snapshots.
- Active agent cap: `MAX_AGENTS = 10`.
- Subagents complete with `agent_finish`, notifying parent through in-memory message bus.
- Waiting behavior is explicit via `wait_for_message`.

### 7) Findings, Dedup, and Scan Completion

Reporting path:

- `create_vulnerability_report` validates mandatory fields and CVSS vector params.
- LLM-based dedupe (`esprit/llm/dedupe.py`) runs against existing tracer reports.
- Accepted reports are persisted to tracer state and disk (`esprit_runs/<run>/vulnerabilities`).

Completion path:

- Root-only `finish_scan` validates:
  - required final sections,
  - no active agents,
  - white-box remediation completeness (with bounce cap).

### 8) Key Gaps for Requested Goals

For mobile pentest:

- No mobile target type in parser/launchpad/scan config.
- No mobile-specific tools.
- Sandbox image lacks mobile reverse/dynamic stack.
- Prompt and scan modes are web/API centric.

For autonomous discovery:

- No persistent hypothesis queue/state machine.
- No novelty scoring or explicit exploration budget.
- No experiment scheduler that turns anomalies into new test campaigns.
- Autonomy depends mostly on prompt wording, not deterministic control logic.

## Track A: Mobile Pentest Implementation

### Goals

- Add first-class mobile target support for `.apk` and `.ipa` artifacts.
- Reuse existing web/API tooling for backend attack surface discovered from mobile apps.
- Keep a safe default: static-first, optional dynamic instrumentation.

### Non-Goals (v1)

- Full iOS runtime bypass automation.
- Device farm orchestration.
- App store crawling.
- Automated certificate pinning bypass or exploit synthesis.

### A1. Target Model and Scan Context

#### Required changes

- `esprit/interface/utils.py`
  - Extend `infer_target_type`:
    - `.apk` -> `mobile_app` with `platform=android`
    - `.ipa` -> `mobile_app` with `platform=ios`
    - optional directory heuristics for unpacked bundles
- `esprit/interface/main.py`
  - Accept and build `targets_info` containing mobile entries.
- `esprit/interface/launchpad.py`
  - Add mobile preview text and validation guidance.
- `esprit/agents/EspritAgent/esprit_agent.py`
  - Include `Mobile Apps:` section in task description.

#### Target schema proposal

```json
{
  "type": "mobile_app",
  "details": {
    "target_path": "/absolute/path/app.apk",
    "platform": "android",
    "workspace_subdir": "app-mobile"
  },
  "original": "/absolute/path/app.apk"
}
```

### A2. Source/Artifact Staging in Runtime

Current staging only handles directories (`collect_local_sources` + directory copy).

#### Required changes

- `esprit/interface/utils.py`
  - Add `collect_local_artifacts()` for files and directories.
- `esprit/runtime/docker_runtime.py`
  - Add file copy support (`_copy_local_file_to_container`) for APK/IPA artifacts.
  - Extend `create_sandbox(..., local_sources=...)` semantics to include artifacts list.
- `esprit/runtime/cloud_runtime.py`
  - Mirror payload schema for mobile artifact metadata.

### A3. Sandbox Tooling Profile

#### Required changes (phase-gated)

- `containers/Dockerfile`
  - Phase 1 (static): `apktool`, `jadx`/`jadx-cli`, `aapt`, `binwalk`, `mitmproxy`, `yq`.
  - Phase 2 (dynamic Android): `adb`, `frida-tools`, `objection`.
  - iOS dynamic should remain optional and explicitly gated.
- `docs/tools/sandbox.mdx`
  - Document mobile tool availability and caveats.

### A3b. Mobile Traffic Plumbing (Minimum Viable Dynamic Path)

Static-only analysis is not enough for realistic mobile pentesting because many request shapes
exist only after authenticated runtime flows.

#### Required v1 behavior

- Add a `mobile proxy mode` with clear operator workflow:
  - user routes device/emulator traffic to scanner proxy,
  - scanner captures traffic into the same Caido request/sitemap store used for web scans.
- Persist captured mobile request templates (method/url/headers/body shape/auth context) so
  replay/mutate uses existing `list_requests`, `view_request`, `repeat_request`, `send_request`.
- Keep this operator-assisted in v1 (no bypass automation).

This preserves one unified backend testing substrate instead of creating a separate mobile replay system.

### A3c. iOS Dynamic Constraints (Explicit)

iOS dynamic testing is not symmetric with Android in this architecture.

- Linux sandbox can support iOS static artifact processing, but iOS runtime instrumentation
  generally requires external device/macOS-side tooling.
- In v1:
  - Android dynamic path can be introduced earlier.
  - iOS dynamic remains explicitly gated as external/operator-assisted.

Docs and UX should state this directly to avoid false expectations.

### A3d. MobSF Integration Decision

MobSF can be introduced in two patterns:

1. CLI-only v1:
   - Use `apktool`/`jadx`/custom parsers in current sandbox image.
   - Lowest operational complexity, easiest CI.
2. MobSF sidecar service:
   - Run MobSF independently and integrate via API.
   - Richer analysis but higher infra complexity.

Recommendation: start with CLI-only (pattern 1) for v1. Reserve MobSF as optional upgrade.

### A7. Testing Plan (Mobile)

#### Unit

- Target parsing tests for APK/IPA paths.
- Tool parser tests for mobile static extractor output.

#### Integration

- Fixture APK in tests (small sample, deterministic expected outputs).
- End-to-end scan in docker runtime with mobile target and no crashes.

#### Bench

- Lab apps only:
  - InsecureBankv2 (Android)
  - DVIA-v2 (iOS static focus initially)

Acceptance for v1:

- App artifact ingested and staged in sandbox.
- Endpoint extraction returns structured output.
- Captured mobile runtime traffic (if provided) is queryable through existing proxy tools.
- At least one extracted endpoint is tested through existing web/API path automatically.
- Vulnerability report flow remains unchanged and stable.

### A8. Rollout Plan (Mobile)

- Milestone M1: parsing + staging + static extraction tool.
- Milestone M2: prompt/skills + endpoint-to-API correlation.
- Milestone M3: optional Android dynamic hooks.
- Milestone M4: docs polish and CI coverage.

## Track B: Autonomous Vulnerability Discovery Implementation

### Goals

- Make exploration stateful and hypothesis-driven.
- Reduce repetitive instruction-following loops.
- Increase novel, validated findings per scan.

### Non-Goals (v1)

- Fully unsupervised exploit synthesis.
- Reinforcement learning training pipelines.

### B1. New Discovery State Model

Extend `AgentState` with explicit discovery fields.

#### Proposed fields

- `hypotheses: list[Hypothesis]`
- `experiments: list[Experiment]`
- `anomaly_events: list[AnomalyEvent]`
- `discovery_metrics: dict`
- `evidence_index: dict[str, EvidenceRef]`

`Hypothesis` example:

```json
{
  "id": "hyp_0001",
  "title": "IDOR on invoice endpoint",
  "source": "proxy_anomaly",
  "target": "/api/invoices/{id}",
  "novelty_score": 0.82,
  "impact_score": 0.70,
  "confidence": 0.45,
  "evidence_refs": ["proxy:req_1842", "browser:shot_117", "terminal:exec_77"],
  "status": "queued"
}
```

### B2. Discovery Engine Module

Add `esprit/discovery/` package with deterministic logic.

#### Proposed modules

- `models.py`: dataclasses/pydantic types for hypothesis/experiment/anomaly.
- `signal_extractor.py`: derive signals from tool results (proxy, terminal, browser).
- `hypothesis_generator.py`: convert signals to candidate hypotheses.
- `prioritizer.py`: scoring and ranking.
- `scheduler.py`: select next experiments/subagent tasks.
- `tracker.py`: lifecycle transitions and metrics.
- `evidence_store.py`: normalized references to raw artifacts (request IDs, tool execution IDs, screenshots).
- `event_bus.py`: append-only discovery events consumed by root aggregator.

### B3. Agent Loop Integration

Hook discovery engine into `BaseAgent` without breaking existing tool flow.

#### Integration points

- Before LLM call in `_process_iteration`:
  - inject top-N queued hypotheses as structured context block.
- After tool results in `_execute_actions`:
  - parse structured observations into events and update anomaly/hypothesis state.
- Before finishing:
  - verify no high-priority hypotheses remain untested (with configurable ceiling).

### B4. Experiment Scheduling via Existing Agent Graph

Use current `create_agent` orchestration, but with deterministic task generation.

Scheduler behavior:

1. Select top ranked hypotheses.
2. Generate focused subagent tasks (one hypothesis per subagent).
3. Attach minimal required skills.
4. Track outcome (`validated`, `falsified`, `inconclusive`).

This keeps compatibility with current graph/messaging model and avoids a second orchestration system.

### B5. Scoring Model (v1)

Simple weighted score:

`priority = 0.35*novelty + 0.30*impact + 0.20*evidence + 0.15*reachability`

Definitions:

- `novelty`: dissimilar to already tested hypotheses and known findings.
- `impact`: estimated blast radius/severity potential.
- `evidence`: strength of anomaly signals.
- `reachability`: likelihood test can be executed with available tooling.

### B6. Novelty and Duplicate Control

Reuse and extend existing dedupe ideas.

- Keep `create_vulnerability_report` dedupe as report-level final guard.
- Add pre-report hypothesis dedupe based on:
  - endpoint
  - parameter
  - vulnerability class
  - root-cause fingerprint

This prevents subagent churn on the same issue.

### B6b. Evidence-First Reproducibility

Each hypothesis and experiment must reference raw evidence artifacts.

- Required evidence pointers:
  - proxy request IDs,
  - tool execution IDs,
  - screenshots / terminal outputs where relevant.
- Discovery summaries should be renderings of evidence-backed events, not free-text-only judgments.

This keeps the autonomy loop auditable and reduces "LLM vibe" drift.

### B7. Safety and Budget Guards

Add hard controls to avoid autonomous runaway.

- Max new hypotheses per iteration.
- Max concurrent experiments per root agent.
- Per-target cooldown when repeated failures occur.
- Respect existing `MAX_AGENTS` and iteration caps.

### B7b. Tool-Layer Scope Enforcement (Mandatory)

Autonomy should not rely only on prompt instructions for scope.

- Enforce scope in execution layer for outbound testing tools:
  - `send_request`,
  - `repeat_request`,
  - any new mobile correlation/replay tool.
- Block or warn on out-of-scope hosts unless explicitly allowed by scan config.
- Prefer deriving allowed hosts from target definitions + proxy scope rules.

### B8. Telemetry and Persistence

Extend tracer persistence for discovery observability.

#### Required changes

- `esprit/telemetry/tracer.py`
  - Persist `hypotheses.json`, `experiments.json`, `anomalies.json` in run dir.
  - Add summary metrics:
    - hypothesis conversion rate
    - novelty ratio
    - validated finding rate
  - Persist evidence index and event logs.

Concurrency note:

- Do not let every subagent write discovery JSON directly.
- Use append-only in-memory events per agent and root-agent aggregation for persistence.
- Add locking around shared tracer structures where needed.

### B9. Testing Plan (Autonomous Discovery)

#### Unit

- Signal extraction for proxy/terminal/browser outputs.
- Prioritizer ordering and dedupe correctness.
- Scheduler respects agent cap and budget limits.
- Scope enforcement checks reject out-of-scope replay attempts.

#### Integration

- Simulated scan traces to verify:
  - hypotheses created,
  - experiments spawned,
  - outcomes fed back,
  - final reports deduped,
  - evidence refs remain valid end-to-end.

#### Benchmark

Run on controlled labs only (no production targets):

- OWASP Juice Shop
- InsecureBankv2
- DVIA-v2 (for static/mobile-driven API hypotheses)

Track over baseline:

- Unique validated vulnerabilities per scan.
- False positive ratio.
- Time-to-first-validated finding.
- Coverage of distinct attack surfaces.

### B10. Rollout Plan (Autonomous Discovery)

- Milestone D1: state model + tracer persistence.
- Milestone D2: signal extraction + hypothesis generation.
- Milestone D3: prioritizer + scheduler + subagent task templates.
- Milestone D4: finish-time guardrails + benchmark gating in CI.

## Combined Delivery Sequence

Recommended execution order to reduce risk:

1. Mobile M1 (target parsing/staging/static extraction).
2. Discovery D1-D2 (state and signal fundamentals).
3. Mobile M2 (API correlation into existing web toolchain).
4. Discovery D3-D4 (true autonomous scheduling and guardrails).
5. Mobile M3-M4 (optional dynamic Android, docs/CI hardening).

This sequence ensures both tracks reuse and strengthen the existing web pipeline instead of forking architecture.

## Definition of Done

Both tracks are done when:

- Mobile targets run end-to-end without custom operator intervention.
- Discovery engine autonomously proposes, prioritizes, and executes non-trivial hypotheses.
- Report quality remains stable (dedupe, reproducibility, CVSS consistency).
- CI includes unit + integration coverage for new code paths.
- Existing web/API workflows remain backward compatible.
